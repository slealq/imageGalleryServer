# Unified Image Clustering System

A comprehensive, professionally-designed clustering solution that addresses the fundamental problems with naive clustering approaches and provides multiple algorithms with intelligent parameter optimization.

## üéØ What This Solves

**Original Problem:** Simple clustering often creates one massive cluster (80%+ of images) with many tiny clusters - not useful for organizing image collections.

**Our Solution:** Multi-algorithm approach with smart evaluation metrics that produces balanced, meaningful clusters.

## ‚ú® Latest Improvements

### üñºÔ∏è Full Image Preservation
- **Default letterbox preprocessing** - preserves entire image content without cropping
- **No more missing details** - center crop mode was losing important visual information
- **Consistent aspect ratios** - all images padded to square while maintaining original content

### üß† Smart Embedding Reuse
- **Automatic compatibility checking** - detects when existing embeddings match your configuration
- **Configuration-specific storage** - embeddings organized by model/resolution/preprocessing
- **Massive time savings** - reuse embeddings across multiple clustering runs
- **Intelligent fallback** - searches all configurations for compatible embeddings

### üß™ Multi-Configuration Experiments
- **Test multiple models simultaneously** - ViT-B-32, ViT-L-14, ViT-H-14 automatically
- **Compare different resolutions** - 224px, 336px, 384px in one command
- **Reuse existing embeddings** - experiments intelligently skip already-generated configurations  
- **Comprehensive comparison** - see which model/resolution combination works best for your data

### ‚ö° Efficient Dataset Sampling
- **Sample BEFORE extraction** - only extracts embeddings from selected subset (e.g., 1% of dataset)
- **Massive time savings** - 50x faster for small samples, no wasted computation
- **Smart preservation** - always includes target image and validation pairs in sample
- **Reproducible sampling** - use `--random-seed` for consistent subsets across runs

---

## üß† What `unified_clustering.py` Does

### Core Innovation: Smart Algorithm Selection & Parameter Optimization

Instead of blindly testing hundreds of parameter combinations (like the old 30+ hour approach), `unified_clustering.py` uses intelligent strategies:

1. **Analyzes your dataset** (size, dimensionality) to predict optimal parameters
2. **Tests multiple clustering algorithms** with carefully selected parameter ranges
3. **Evaluates results using composite scoring** (balance + quality + noise handling)
4. **Saves the top 10 best approaches** for you to choose from

### Supported Algorithms

| Algorithm | Best For | Speed | Cluster Balance |
|-----------|----------|-------|-----------------|
| **K-Means** | Even distributions, guaranteed cluster counts | Fast | Excellent |
| **HDBSCAN** | Varying densities, automatic cluster detection | Medium | Good |
| **DBSCAN** | Dense regions, noise handling | Medium | Fair |
| **Gaussian Mixture** | Overlapping clusters, probabilistic assignment | Fast | Good |

### Intelligent Parameter Selection

**For K-Means:**
- Calculates optimal cluster count based on dataset size and target cluster size
- Tests around the calculated optimum (0.5x, 0.75x, 1x, 1.25x, 1.5x)

**For HDBSCAN:**
- Selects min_cluster_size based on desired cluster balance
- Tests key epsilon values that typically work well for image embeddings

**For DBSCAN:**
- Uses cosine similarity (perfect for CLIP embeddings)
- Tests eps values in the range that works for normalized embeddings

**For Gaussian Mixture:**
- Estimates components based on dataset size and target distribution
- Uses efficient covariance types for speed

---

## ‚ö° Execution Modes

### `--mode fast` (Recommended)
- **Time:** 30-60 minutes
- **Combinations:** 15-30 carefully selected parameters
- **Best for:** Most users, getting good results quickly
- **Approach:** Tests the most promising parameters for each algorithm

### `--mode balanced` 
- **Time:** 1-3 hours
- **Combinations:** 50-80 parameters
- **Best for:** When you need better coverage but don't want to wait all day
- **Approach:** Broader parameter search with smart sampling

### `--mode comprehensive`
- **Time:** 4-12 hours  
- **Combinations:** 200+ parameters
- **Best for:** Research, when you need to explore all possibilities
- **Approach:** Thorough search across parameter space

### `--mode gpu`
- **Time:** 15-45 minutes (with GPU)
- **Speedup:** 10-50x for supported algorithms
- **Requirements:** NVIDIA GPU + CUDA + cuML library
- **Best for:** Large datasets when you have GPU available

---

## üèóÔ∏è Professional Software Architecture

Built using proven design patterns for maintainability and extensibility:

### Design Patterns Used
- **Strategy Pattern**: Each clustering algorithm is a pluggable strategy
- **Factory Pattern**: ClusteringStrategyFactory creates algorithm instances
- **Template Method**: Common workflow in UnifiedClusteringManager
- **Configuration Object**: ClusteringConfig centralizes all settings

### Key Components
```python
UnifiedClusteringManager      # Main orchestrator
‚îú‚îÄ‚îÄ DataLoader               # Handles embeddings/image paths
‚îú‚îÄ‚îÄ ClusteringStrategy       # Abstract base for algorithms
‚îÇ   ‚îú‚îÄ‚îÄ KMeansStrategy      # Smart K-Means implementation
‚îÇ   ‚îú‚îÄ‚îÄ HDBSCANStrategy     # Optimized HDBSCAN
‚îÇ   ‚îú‚îÄ‚îÄ DBSCANStrategy      # Cosine-similarity DBSCAN
‚îÇ   ‚îî‚îÄ‚îÄ GaussianMixture     # Probabilistic clustering
‚îú‚îÄ‚îÄ ClusteringEvaluator     # Multi-metric quality assessment
‚îî‚îÄ‚îÄ ResultsManager          # Output handling and ranking
```

### Why This Architecture Matters
- **Extensible**: Easy to add new algorithms
- **Testable**: Each component can be tested independently  
- **Maintainable**: Clear separation of concerns
- **Configurable**: Behavior controlled by configuration objects
- **Reusable**: Components can be used independently

---

## üìä Advanced Evaluation System

### Composite Scoring (0-1, higher is better)
```
Composite Score = 0.3 √ó Silhouette + 0.4 √ó Balance + 0.3 √ó Noise_Penalty
```

**Silhouette Score**: How well-separated clusters are (classic ML metric)
**Balance Score**: How evenly distributed cluster sizes are (prevents massive clusters)
**Noise Penalty**: Penalizes approaches that label too many points as noise

### Why This Matters
Simple clustering often optimizes only silhouette score, leading to:
- One massive cluster (easy to achieve high silhouette)
- Many tiny clusters (technically "well-separated")
- Not useful for organizing images

Our composite score ensures **practical utility**.

---

## üöÄ Quick Start Guide

copy-paste the following: 

```bash
wsl
sudo mount -t drvfs D: /mnt/d
# pass 9571
cd
source unsloth/bin/activate
cd /mnt/c/playground/imageGalleryServer/helper_scripts
export LD_LIBRARY_PATH="$VIRTUAL_ENV/lib/python3.12/site-packages/nvidia/cuda_runtime/lib:$LD_LIBRARY_PATH" 
```

restarting WSL

```bash
wsl --shutdown
```

running for 1% of dataset (preserves full images with letterbox)
```bash
python streamlined_image_clustering.py --target-image "KayLovely_MilaMonet_RyanDriller__August_20_2022_4800_1347_121_crop_1024.jpg" --input-dir "/mnt/d/TEST/images" --output-dir "/mnt/c/Users/stuar/Downloads/image_clustering_results" --mode fast --top-results 5 --skip-embeddings --dataset-sample 5.0  
```

### ‚ú® NEW: Smart Embedding Reuse & Experiment Mode

**Preserve Full Images (No Cropping):**
```bash
# Letterbox preprocessing (default) - preserves entire image, pads to square
python streamlined_image_clustering.py --target-image "your_image.jpg" --preprocessing-mode letterbox

# Compare preprocessing modes
python streamlined_image_clustering.py --target-image "your_image.jpg" --preprocessing-mode resize     # distorts aspect ratio
python streamlined_image_clustering.py --target-image "your_image.jpg" --preprocessing-mode center_crop  # crops content
python streamlined_image_clustering.py --target-image "your_image.jpg" --preprocessing-mode letterbox    # preserves all content (recommended)
```

**Smart Embedding Reuse:**
```bash
# First run - generates embeddings
python streamlined_image_clustering.py --target-image "your_image.jpg" --clip-model ViT-L-14 --input-resolution 224

# Second run - automatically reuses compatible embeddings (same model, resolution, preprocessing)
python streamlined_image_clustering.py --target-image "your_image.jpg" --clip-model ViT-L-14 --input-resolution 224 --skip-embeddings

# Force regeneration even if compatible embeddings exist
python streamlined_image_clustering.py --target-image "your_image.jpg" --force-regenerate
```

**Multi-Configuration Experiments:**
```bash
# Test multiple models and resolutions automatically
python streamlined_image_clustering.py \
  --target-image "your_image.jpg" \
  --experiment-mode \
  --experiment-models ViT-B-32:openai ViT-L-14:openai ViT-H-14:openai \
  --experiment-resolutions 224 336 384

# Quick experiment with default configurations
python streamlined_image_clustering.py --target-image "your_image.jpg" --experiment-mode
```

### Option 1: Auto-Detection (Easiest)
```bash
cd imageGalleryServer/helper_scripts

# Automatically finds your embeddings and runs optimal clustering
python unified_clustering.py --mode fast
```

### Option 2: Specify Files
```bash
python unified_clustering.py embeddings.npy image_paths.json --mode fast
```

### Option 3: GPU Acceleration
```bash
# Install GPU support first
pip install cupy-cuda12x cuml-cu12

# Run with GPU acceleration
python unified_clustering.py --mode gpu
```

### Option 4: Custom Configuration
```bash
python unified_clustering.py \
  --mode balanced \
  --algorithms kmeans gaussian_mixture \
  --target-cluster-size 150 \
  --max-clusters 200 \
  --output-dir my_clustering_results
```

---

## üìÅ Output Structure

```
clustering_results/
‚îú‚îÄ‚îÄ clustering_summary.json       # Overall results and rankings
‚îú‚îÄ‚îÄ all_results.pkl               # Complete data for analysis
‚îú‚îÄ‚îÄ rank_01_kmeans/               # Best performing approach
‚îÇ   ‚îú‚îÄ‚îÄ cluster_labels.npy        # Cluster assignments for each image
‚îÇ   ‚îî‚îÄ‚îÄ clustering_info.json      # Parameters and metrics
‚îú‚îÄ‚îÄ rank_02_gaussian_mixture/     # Second best approach
‚îÇ   ‚îú‚îÄ‚îÄ cluster_labels.npy
‚îÇ   ‚îî‚îÄ‚îÄ clustering_info.json
‚îî‚îÄ‚îÄ rank_03_hdbscan/              # Third best approach
    ‚îú‚îÄ‚îÄ cluster_labels.npy
    ‚îî‚îÄ‚îÄ clustering_info.json
```

### How to Use Results
1. **Check `clustering_summary.json`** for overview of all results
2. **Use `rank_01_*/cluster_labels.npy`** - these are the best cluster assignments
3. **Load cluster assignments:**
   ```python
   import numpy as np
   labels = np.load('clustering_results/rank_01_kmeans/cluster_labels.npy')
   # labels[i] gives cluster ID for image i
   ```

---

## üéØ When to Use Each Script

### Use `unified_clustering.py` when:
- ‚úÖ You have pre-computed CLIP embeddings
- ‚úÖ You want the best possible clustering quality
- ‚úÖ You need balanced cluster distributions  
- ‚úÖ You have time for proper optimization (30+ minutes)
- ‚úÖ You want to compare multiple algorithms

### Use `categorization.py` when:
- ‚úÖ You have raw images (no embeddings yet)
- ‚úÖ You want a simple, one-step solution
- ‚úÖ You're okay with HDBSCAN-only results
- ‚úÖ You need results in 10-30 minutes

---

## üîß Installation & Requirements

### Core Requirements
```bash
pip install scikit-learn hdbscan numpy tqdm matplotlib seaborn
```

### GPU Acceleration (Optional)
```bash
# Check your CUDA version first
nvidia-smi

# For CUDA 12.x
pip install cupy-cuda12x cuml-cu12

# For CUDA 11.x  
pip install cupy-cuda11x cuml-cu11
```

### For `categorization.py` (if starting from raw images)
```bash
pip install torch open-clip-torch pillow
```

---

## üìà Real Performance Results

Based on testing with 48,642 images:

### Algorithm Performance Ranking
1. **Gaussian Mixture (20 components)**: Score 0.597, Balance 0.313
2. **Gaussian Mixture (60 components)**: Score 0.547, Balance 0.148  
3. **K-Means (72 clusters)**: Score 0.546, Balance 0.140
4. **K-Means (16 clusters)**: Score 0.535, Balance 0.092

### Key Insights
- **Gaussian Mixture** works exceptionally well for image embeddings
- **K-Means** provides most balanced cluster sizes
- **HDBSCAN** struggles with high-dimensional CLIP embeddings (too much noise)
- **Optimal cluster count** for 48K images: 20-80 clusters

---

## ‚ö° Performance Comparison

| Approach | Time | Quality | Cluster Balance | Best For |
|----------|------|---------|-----------------|----------|
| Old `advanced_clustering.py` | 30+ hours | High | Medium | Research only |
| `unified_clustering.py --mode fast` | 30-60 min | High | High | ‚≠ê **Recommended** |
| `unified_clustering.py --mode gpu` | 15-45 min | High | High | Large datasets |
| `categorization.py` | 10-30 min | Medium | Medium | Quick & simple |

---

## üÜò Troubleshooting

### "No embeddings found"
```bash
# Option 1: Let it auto-detect
python unified_clustering.py --mode fast

# Option 2: Specify manually  
find . -name "embeddings.npy"
find . -name "image_paths.json"
python unified_clustering.py /path/to/embeddings.npy /path/to/image_paths.json
```

### GPU Issues
```bash
# Check if CUDA is available
nvidia-smi

# Install correct cuPy version
# For CUDA 12.x: pip install cupy-cuda12x cuml-cu12
# For CUDA 11.x: pip install cupy-cuda11x cuml-cu11

# Or disable GPU
python unified_clustering.py --mode fast --no-gpu
```

### Memory Issues
```bash
# Reduce parallel jobs
python unified_clustering.py --mode fast --n-jobs 2

# Or run sequentially
python unified_clustering.py --mode fast --no-parallel
```

### Taking Too Long
```bash
# Always start with fast mode
python unified_clustering.py --mode fast

# Limit algorithms
python unified_clustering.py --mode fast --algorithms kmeans gaussian_mixture

# Set maximum combinations
python unified_clustering.py --mode balanced --max-combinations 30
```

---

## üéâ Success Story

**Before:** 82.3% of images in one massive cluster, hundreds of tiny clusters
**After:** 20-80 balanced clusters with 100-500 images each, perfect for image gallery organization

The unified clustering system solved the fundamental problem of naive clustering and provides professional-grade results in a fraction of the time. 